# Green Thread Runtime: Implementation Phases

## Phase 1: Basic Threading
This phase establishes the foundation of the green thread runtime by implementing lightweight user-space threads. Each thread has its own stack and execution context, allowing multiple execution flows within a single OS thread. The core components include a context switching mechanism using assembly to save and restore CPU registers, enabling cooperative multitasking. Threads can be created, started, and yield control back to the scheduler. The implementation uses a simple linked list to manage thread states (READY, RUNNING, FINISHED) and switches between them using a round-robin scheduler. This phase demonstrates the fundamental concept of userspace threading without OS intervention.

## Phase 2: Thread Management
Building on the basic threading model, this phase introduces comprehensive thread lifecycle management. It implements thread cleanup handlers to properly release resources when threads terminate, preventing memory leaks. The scheduler is enhanced to handle multiple threads with different priorities, and thread-local storage is added to support per-thread variables. The implementation includes thread naming and identification for better debugging. The scheduler now maintains separate queues for different priority levels, allowing high-priority threads to preempt lower-priority ones. This phase also introduces thread safety mechanisms to prevent race conditions in the scheduler itself.

## Phase 3: Preemptive Scheduling
This phase transforms the cooperative multitasking model into a preemptive one using POSIX signals and timers. A high-precision timer (using `timer_create` and `SIGALRM`) interrupts the running thread at regular intervals, forcing a context switch. The scheduler is enhanced to handle these interrupts safely, saving the interrupted thread's state and restoring the next thread's state. The implementation includes mechanisms to prevent signal handler reentrancy and to handle nested interrupts. This enables true concurrency and prevents any single thread from monopolizing the CPU, making the system more responsive and fair.

## Phase 4: Synchronization
To enable safe communication between threads, this phase implements synchronization primitives. The Mutex class provides mutual exclusion using atomic operations and a wait queue for blocked threads. Condition variables allow threads to wait for specific conditions to be met, with proper handling of spurious wakeups. The implementation includes deadlock detection and prevention mechanisms. These primitives are implemented without busy waiting, allowing the CPU to be used productively while threads wait. The phase also includes various synchronization patterns like reader-writer locks and barriers, all built on top of the basic mutex and condition variable primitives.

## Phase 5: I/O Event Notification
This phase adds support for efficient I/O operations through an event-driven architecture. It implements an I/O multiplexing system using `epoll` to monitor multiple file descriptors for read/write readiness. The event loop integrates with the thread scheduler, allowing threads to block on I/O operations without wasting CPU cycles. When I/O becomes available, the corresponding thread is made runnable again. This enables the creation of highly concurrent network servers that can handle thousands of simultaneous connections with minimal overhead. The implementation includes support for timers and idle callbacks, making it suitable for a wide range of I/O-bound applications.

## Phase 6: Task Parallelism
The final phase introduces a high-level task parallelism model on top of the green thread runtime. It features a work-stealing scheduler that automatically distributes tasks across multiple worker threads, maximizing CPU utilization. The task system supports futures and continuations, allowing for clean expression of parallel algorithms. The implementation includes task dependencies, task groups, and parallel iteration constructs. The work-stealing algorithm ensures good load balancing by allowing idle workers to "steal" tasks from busy workers' queues. This phase makes it easy to write parallel programs that automatically scale with the available CPU cores while maintaining the simplicity of the sequential programming model.
